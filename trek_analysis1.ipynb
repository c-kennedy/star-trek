{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with the *Star Trek* corpus, part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a longtime Trekkie, it seemed like a neat idea to use the corpus of *Star Trek* scripts to get some experience with pandas and, hopefully, some natural language processing with spaCy. This is possible thanks to the efforts of chakoteya.net (which hosts HTML versions of the scripts) and Gary Broughton, who scraped them and put them in a much friendlier JSON format. I wrote a module called ```trekparser``` (available in this repo) to break them down even further into a big DataFrame of lines of dialogue. This DataFrame is also available in this repo, in csv format as ```table_of_lines.csv```.\n",
    "\n",
    "For the first in what I hope to be a several-part series of posts, I'll poke around this fairly large dataset, extract some fun facts, and possibly even gain some insights into large-scale properties of the Trek corpus.\n",
    "\n",
    "Two notes on terminology before we start. First, we've got a collision between TV series and pandas Series; the capitalization will distinguish them. Second, since they're going to be used extensively, the abbrevations used for the various series stand for: TOS (The Original Series, 1966-69), TAS (The Animated Series, 1973-74), TNG (The Next Generation, 1987-94), DS9 (Deep Space 9, 1993-99), VOY (Voyager, 1995-2001), and ENT (Enterprise, 2001-05). Of course, you could also get the years from the airdates in the episodes table!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the relevant stuff, including the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "episodes = pd.read_csv('episode_index.csv', index_col=['series','episode'])\n",
    "lines = pd.read_csv('table_of_lines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, do these DataFrames look like they have the right stuff? Keep in mind that the episodes are indexed first by series lexicographically, so DS9 should come up first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good so far. How about some general facts about the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d total lines of dialogue across all series.' % (len(lines)))\n",
    "print('There are %d distinct named characters.' % (len(lines.character.unique())))\n",
    "print('The 73rd episode of TNG is %s, which first aired on %s.' %\n",
    "      (episodes.loc[('TNG',72)].title, episodes.loc[('TNG',72)].airdate))\n",
    "longest = lines.loc[lines.line_number==lines.line_number.max()].squeeze()\n",
    "#note: this will only work if there's a unique longest scene\n",
    "print('''The longest scene in the franchise by number of lines is scene %d of the %s episode %s, which takes place \n",
    "      in the %s. It has %d lines!''' %\n",
    "     (longest.scene_number, longest.series, longest.ep_name, longest.scene_loc, longest.line_number+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all the times Dr. McCoy says, \"He's dead, Jim\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.loc[(lines.character=='MCCOY') & (lines.line.str.contains(\"e's dead, Jim\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see how messy the formatting got, how many lines end with something other than punctuation? Seems like a good guess that some of the lines that end on a letter got cut off somehow, and might need further scrutiny..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines.loc[~lines.line.str.endswith(('.','!','?',')'))])/len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, though that is still 5194 lines of dialogue (and if you look in the value counts for ```lines.line.str.slice(-1)```, you'll see that there's some places where something wonky probably happened, like the 32 lines that end with ']' or the 2 lines that end with '}')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's get an example of the potential multi-index for the big DataFrame in use. As suggested in the readme, let's reset the index to be by series, ep_number, scene_number, and line_number, and then pull out a line completely at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.set_index(['series','ep_number','scene_number','line_number'], inplace=True)\n",
    "lines.loc[('TNG',72,46,16)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
